{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from bs4 import BeautifulSoup\n",
    "#https://pypi.org/project/beautifulsoup4/\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "#https://radimrehurek.com/gensim/models/word2vec.html\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "#https://pypi.org/project/multiprocess/\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "from sklearn.model_selection  import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "data_folder = 'D:\\\\text-classification\\\\'\n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "#https://kdd.ics.uci.edu/databases/reuters21578/README.txt\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "#features\n",
    "num_features = 500\n",
    "# Limit\n",
    "document_max_num_words = 100\n",
    "#categories\n",
    "selected_categories = ['pl_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), category_files[category_prefix][0],0])\n",
    "\n",
    "#dataframe\n",
    "news_categories = pd.DataFrame(data=category_data, columns=['Name', 'Type', 'Newslines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frequencies(categories):\n",
    "    for category in categories:\n",
    "        idx = news_categories[news_categories.Name == category].index[0]\n",
    "        f = news_categories.get_value(idx, 'Newslines')\n",
    "        news_categories.set_value(idx, 'Newslines', f+1)\n",
    "    \n",
    "def to_category_vector(categories, target_categories):\n",
    "    l = len(target_categories)\n",
    "    vector = np.zeros(l).astype(float)\n",
    "    \n",
    "    for i in range(l):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n",
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    \n",
    "    with open(data_folder + file_name, 'r') as file:\n",
    "        content = BeautifulSoup(file.read().lower())\n",
    "        \n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "            \n",
    "            #Id\n",
    "            document_id = newsline['newid']\n",
    "            \n",
    "            #text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            document_body = unescape(document_body)\n",
    "            \n",
    "            #categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "            \n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "                \n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "                \n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "                \n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "                \n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "                \n",
    "            # Create new document    \n",
    "            update_frequencies(document_categories)\n",
    "            \n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = to_category_vector(document_categories, selected_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name           Type  Newslines\n",
      "296           pl_usa         Places      12542\n",
      "35           to_earn         Topics       3987\n",
      "0             to_acq         Topics       2448\n",
      "293            pl_uk         Places       1489\n",
      "219         pl_japan         Places       1138\n",
      "166        pl_canada         Places       1104\n",
      "73       to_money-fx         Topics        801\n",
      "28          to_crude         Topics        634\n",
      "45          to_grain         Topics        628\n",
      "302  pl_west-germany         Places        567\n",
      "126         to_trade         Topics        552\n",
      "55       to_interest         Topics        513\n",
      "191        pl_france         Places        469\n",
      "587            or_ec  Organizations        349\n",
      "158        pl_brazil         Places        332\n",
      "130         to_wheat         Topics        306\n",
      "108          to_ship         Topics        305\n",
      "145     pl_australia         Places        270\n",
      "19           to_corn         Topics        254\n",
      "172         pl_china         Places        223\n"
     ]
    }
   ],
   "source": [
    "news_categories.sort_values(by='Newslines', ascending=False, inplace=True)\n",
    "print(news_categories.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "newsline_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "for key in document_X.keys():\n",
    "    newsline_documents.append(tokenize(document_X[key]))\n",
    "\n",
    "number_of_documents = len(document_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Create new Gensim Word2Vec model\n",
    "#https://radimrehurek.com/gensim/models/word2vec.html\n",
    "#https://code.google.com/archive/p/word2vec/\n",
    "w2v_model = Word2Vec(newsline_documents, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "num_categories = len(selected_categories)\n",
    "X = np.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(float)\n",
    "Y = np.zeros(shape=(number_of_documents, num_categories)).astype(float)\n",
    "\n",
    "empty_word = np.zeros(num_features).astype(float)\n",
    "\n",
    "for idx, document in enumerate(newsline_documents):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word\n",
    "\n",
    "for idx, key in enumerate(document_Y.keys()):\n",
    "    Y[idx, :] = document_Y[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shrey\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15104 samples, validate on 6474 samples\n",
      "Epoch 1/5\n",
      "15104/15104 [==============================] - 1593s 105ms/step - loss: 0.5959 - acc: 0.6751 - val_loss: 0.4102 - val_acc: 0.8706\n",
      "Epoch 2/5\n",
      "15104/15104 [==============================] - 1961s 130ms/step - loss: 0.4249 - acc: 0.8599 - val_loss: 0.4357 - val_acc: 0.8462\n",
      "Epoch 3/5\n",
      "15104/15104 [==============================] - 2448s 162ms/step - loss: 0.4377 - acc: 0.8424 - val_loss: 0.4586 - val_acc: 0.8268\n",
      "Epoch 4/5\n",
      "15104/15104 [==============================] - 1262s 84ms/step - loss: 0.4446 - acc: 0.8398 - val_loss: 0.4435 - val_acc: 0.8483\n",
      "Epoch 5/5\n",
      "15104/15104 [==============================] - 1436s 95ms/step - loss: 0.4492 - acc: 0.8256 - val_loss: 0.4447 - val_acc: 0.8133\n",
      "6474/6474 [==============================] - 12s 2ms/step\n",
      "Score: 0.4447\n",
      "Accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, Y_train, batch_size=512, epochs=5, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=512)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
